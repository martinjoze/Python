# -*- coding: utf-8 -*-
"""ML Task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P_SkfeE3PASCN3n5Jl3TJ4ntRJhwlISy

**#Machine Learning Task 2 - Classification**
"""

#Machine Learning Task 2 - Classification
Build different classification models from historical data of patients and their responses to
different medications. Then you use the trained algorithms to predict the class of an unknown
patient or to find a proper drug for a new patient.
#About the dataset
Imagine that you are a medical researcher compiling data for a study. You have collected data about
a set of patients, all of whom suffered from the same illness. During their course of treatment, each
patient responded to one of 5 medications, Drug A, Drug B, Drug c, Drug x and y.
Part of your job is to build a model to find out which drug might be appropriate for a future patient
with the same illness. The feature sets of this dataset are Age, Sex, Blood Pressure, and Cholesterol
of patients, and the target is the drug that each patient responded to.
You can use the training part of the dataset to build a logistic regression, decision tree, Random
Forest, KNN, SVM, and Naive Bayes Classifiers and then use it to predict the class of an unknown
patient, or to prescribe it to a new patient.

!wget -O drug200.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/drug200.csv

#import basic libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets

data=pd.read_csv("/content/drug200.csv")
data

print(data.info())

data.Drug.value_counts()

data.Sex.value_counts()

data.BP.value_counts()

data.Cholesterol.value_counts()

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
X = data.drop(["Drug"], axis=1)
y = data["Drug"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

X_train = pd.get_dummies(X_train)
X_test = pd.get_dummies(X_test)

X_train.head()

#Encoding categorical variables
from sklearn.preprocessing import LabelEncoder
import pandas as pd
data=pd.read_csv("/content/drug200.csv")
data.fillna(data.mean(),inplace=True)
label_encoder=LabelEncoder()
data['Sex']=label_encoder.fit_transform(data['Sex'])
print(data.head())



import pandas as pd
from sklearn.model_selection import train_test_split
data=pd.read_csv("/content/drug200.csv")
#Split the data into features (X) and target variable (y)
X=data[['Age','Sex','BP','Cholesterol']]
y=data['Drug']
#split into training and testing test
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
print("X_train shape:",X_train.shape)
print("X_test shape:",X_test.shape)
print("y_train shape:",y_train.shape)
print("y_test shape:",y_test.shape)

bin_age = [0, 19, 29, 39, 49, 59, 69, 80]
category_age = ['<20s', '20s', '30s', '40s', '50s', '60s', '>60s']
data['Age_binned'] = pd.cut(data['Age'], bins=bin_age, labels=category_age)
data = data.drop(['Age'], axis = 1)

#Logistic Regression
from sklearn.linear_model import LogisticRegression
LRclassifier = LogisticRegression(solver='liblinear', max_iter=5000)
LRclassifier.fit(X_train, y_train)

y_pred = LRclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
LRAcc = accuracy_score(y_pred,y_test)
print('Logistic Regression accuracy is: {:.2f}%'.format(LRAcc*100))

#Decision Tree
from sklearn.tree import DecisionTreeClassifier
DTclassifier = DecisionTreeClassifier(max_leaf_nodes=20)
DTclassifier.fit(X_train, y_train)

y_pred = DTclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
DTAcc = accuracy_score(y_pred,y_test)
print('Decision Tree accuracy is: {:.2f}%'.format(DTAcc*100))

#Random Forest
from sklearn.ensemble import RandomForestClassifier

RFclassifier = RandomForestClassifier(max_leaf_nodes=30)
RFclassifier.fit(X_train, y_train)

y_pred = RFclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
RFAcc = accuracy_score(y_pred,y_test)
print('Random Forest accuracy is: {:.2f}%'.format(RFAcc*100))

#K-Nearest Neighbors (KNN)
from sklearn.neighbors import KNeighborsClassifier
KNclassifier = KNeighborsClassifier(n_neighbors=20)
KNclassifier.fit(X_train, y_train)

y_pred = KNclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
KNAcc = accuracy_score(y_pred,y_test)
print('K Neighbours accuracy is: {:.2f}%'.format(KNAcc*100))

#Support Vector Machine
from sklearn.svm import SVC
SVCclassifier = SVC(kernel='linear', max_iter=251)
SVCclassifier.fit(X_train, y_train)

y_pred = SVCclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
SVCAcc = accuracy_score(y_pred,y_test)
print('SVC accuracy is: {:.2f}%'.format(SVCAcc*100))

#Naive Bayes Classifier
from sklearn.naive_bayes import GaussianNB
NBclassifier2 = GaussianNB()
NBclassifier2.fit(X_train, y_train)

y_pred = NBclassifier2.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
NBAcc2 = accuracy_score(y_pred,y_test)
print('Gaussian Naive Bayes accuracy is: {:.2f}%'.format(NBAcc2*100))

#Model Comparison ðŸ‘€
compare = pd.DataFrame({'Model': ['Logistic Regression', 'K Neighbors', 'SVM', 'Gaussian NB', 'Decision Tree', 'Random Forest'],
                        'Accuracy': [LRAcc*100, KNAcc*100, SVCAcc*100, NBAcc2*100, DTAcc*100, RFAcc*100]})
compare.sort_values(by='Accuracy', ascending=False)
